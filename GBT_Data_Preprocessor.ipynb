{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047f1638",
   "metadata": {
    "id": "f8301228",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79a14fb",
   "metadata": {
    "id": "69dda83b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./processed_facial_data/X_train.pickle', 'rb'))\n",
    "y_train = pickle.load(open('./processed_facial_data/y_train.pickle', 'rb'))\n",
    "X_test = pickle.load(open('./processed_facial_data/X_test.pickle', 'rb'))\n",
    "y_test = pickle.load(open('./processed_facial_data/y_test.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce378101",
   "metadata": {
    "id": "62c9521a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_train = X_train/255.0\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef2f857",
   "metadata": {
    "id": "0898d70a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Attempting to extract features from images through CNN t-SNE\n",
    "# conv2d layers are indexed at [1, 6]\n",
    "model = load_model('./facial_emotion_recognition.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5838cab6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JEaXb2SY5Ubp",
    "outputId": "b24b5d33-7b76-4d57-fd03-a915675cf5e9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in model.layers]\n",
    "layer_outputs = [layer.output for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb527b42",
   "metadata": {
    "id": "EwNEaS9i66IZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_map_model = Model(inputs = model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f257c4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkz8SHUIlGAf",
    "outputId": "bac5f9d5-26d0-494c-dd0c-ca1a5a909d0f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    }
   ],
   "source": [
    "new_img = X_train[0].reshape(1, X_train[0].shape[0], X_train[0].shape[1])\n",
    "features = feature_map_model.predict(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba56ee03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABM0ArLuDpBt",
    "outputId": "327fcd6a-3145-43aa-fd8c-46353a13626d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizing feature shapes\n",
    "# for feature in features:\n",
    "#     print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a16fc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "n2kD421xManh",
    "outputId": "f946dcba-7833-4c64-af1c-92f469fc61e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(X_test[0])\n",
    "# y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f9f257c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 987
    },
    "id": "CZ_a7koIAlIt",
    "outputId": "6013edf9-7da3-4e62-d5f8-1f797ae6dd95",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizing the features from the cnn\n",
    "# first conv2d layer has 32 filters\n",
    "# second conv2d layer has 64 filters\n",
    "# features are arranged as:\n",
    "# features[cnn_layer_index][image_index,dimension1,dimension2,filter_index]\n",
    "# hint: for filter_index, look at the amount of filters you are adding per conv2d layer (32, 64, etc)\n",
    "\n",
    "# for i in range(len(layer_names)):\n",
    "#     if layer_names[i].split('_')[0] == 'activation':\n",
    "#         print(f'{i}: {layer_names[i]}')\n",
    "#         try:\n",
    "#             plt.imshow(features[i][0,:,:,15], cmap='Greys')\n",
    "#             plt.show()\n",
    "#         except:\n",
    "#             print(features[i].shape)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f83778ab",
   "metadata": {
    "id": "ofmG39lMUQQY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# attempting to flatten features from 'activation' layer\n",
    "flattened = features[2][0,:,:,31].ravel()\n",
    "# features[2][0,:,:,31].shape\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4de581",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5Ya5eKuWLVx",
    "outputId": "4cde135f-e007-4f41-ae07-3d89c5467450",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2116,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e9c90c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "# get length of features\n",
    "img = X_train[0].reshape(1,X_train[0].shape[0],X_train[0].shape[1])\n",
    "feature_map_model = Model(inputs = model.input, outputs=layer_outputs)\n",
    "sample = feature_map_model.predict(img)\n",
    "\n",
    "# # defining the training data matrix\n",
    "# train_data = np.ndarray([X_train.shape[0],len(sample), 211], dtype=object)\n",
    "# print(train_data.shape)\n",
    "# print(type(train_data))\n",
    "# print(train_data)\n",
    "# print(train_data[0][26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e0ea688",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# features gets you a (1,dim_1,dim_2,num_filters) shaped data\n",
    "# for the layers that are conv2d, we will arrannge the data as [[filter_1], [filter_2],...,[filter_n]]\n",
    "# features_train[19][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca83ecb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "FOR X_train[0]\n",
      "conv_features length = 32 with conv_features.shape = (32, 2116), total_parameters = 67712\n",
      "conv_features length = 32 with conv_features.shape = (32, 2116), total_parameters = 67712\n",
      "conv_features length = 32 with conv_features.shape = (32, 2116), total_parameters = 67712\n",
      "conv_features length = 32 with conv_features.shape = (32, 529), total_parameters = 16928\n",
      "conv_features length = 32 with conv_features.shape = (32, 529), total_parameters = 16928\n",
      "conv_features length = 64 with conv_features.shape = (64, 529), total_parameters = 33856\n",
      "conv_features length = 64 with conv_features.shape = (64, 529), total_parameters = 33856\n",
      "conv_features length = 64 with conv_features.shape = (64, 529), total_parameters = 33856\n",
      "conv_features length = 64 with conv_features.shape = (64, 121), total_parameters = 7744\n",
      "conv_features length = 64 with conv_features.shape = (64, 121), total_parameters = 7744\n",
      "conv_features length = 64 with conv_features.shape = (64, 121), total_parameters = 7744\n",
      "conv_features length = 64 with conv_features.shape = (64, 121), total_parameters = 7744\n",
      "conv_features length = 64 with conv_features.shape = (64, 121), total_parameters = 7744\n",
      "conv_features length = 64 with conv_features.shape = (64, 25), total_parameters = 1600\n",
      "conv_features length = 64 with conv_features.shape = (64, 25), total_parameters = 1600\n",
      "dense_features length = 1 with dense_features.shape = (1, 1600)\n",
      "dense_features length = 1 with dense_features.shape = (1, 32)\n",
      "dense_features length = 1 with dense_features.shape = (1, 32)\n",
      "dense_features length = 1 with dense_features.shape = (1, 32)\n",
      "dense_features length = 1 with dense_features.shape = (1, 64)\n",
      "dense_features length = 1 with dense_features.shape = (1, 64)\n",
      "dense_features length = 1 with dense_features.shape = (1, 64)\n",
      "dense_features length = 1 with dense_features.shape = (1, 64)\n",
      "dense_features length = 1 with dense_features.shape = (1, 64)\n",
      "dense_features length = 1 with dense_features.shape = (1, 64)\n",
      "dense_features length = 1 with dense_features.shape = (1, 7)\n",
      "dense_features length = 1 with dense_features.shape = (1, 7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# starting with the first two images of the dataset, then we can generalize onto the rest of the images\n",
    "# X_train.shape[0]\n",
    "for j in range(1):\n",
    "    # adding an additional dimension to image (1,dim_1,dim_2)\n",
    "    img = X_train[j].reshape(1, X_train[j].shape[0], X_train[j].shape[1])\n",
    "    # extracting features from the images in training set\n",
    "    features_train = feature_map_model.predict(img)\n",
    "    print(f'FOR X_train[{j}]')\n",
    "    for k in range(len(features_train)):\n",
    "        conv_features = []\n",
    "        dense_features = []\n",
    "        if len(features_train[k].shape) == 4:       # extract features from conv2d layers\n",
    "            for l in range(features_train[k].shape[3]):  # append all conv2d filters (32 or 64 filters) to our conv_features array\n",
    "                conv_features.append( features_train[k][0,:,:,l].ravel() )\n",
    "\n",
    "        elif len(features_train[k].shape) == 2:     # extract features from dense layers\n",
    "            dense_features.append(features_train[k][0])\n",
    "        \n",
    "        \n",
    "        if len(conv_features) != 0:\n",
    "            conv_features = np.array(conv_features)\n",
    "            print(f'conv_features length = {len(conv_features)} with conv_features.shape = {conv_features.shape}, total_parameters = {conv_features.shape[0]*conv_features.shape[1]}')\n",
    "        \n",
    "        if len(dense_features) != 0:\n",
    "            dense_features = np.array(dense_features)\n",
    "            print(f'dense_features length = {len(dense_features)} with dense_features.shape = {dense_features.shape}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15a7c137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# concluding from the above printing, we can easily now picture how our dataset would look like. \n",
    "# We will have 27 different datasets, and we will store them into an array that will have the following shape:\n",
    "# train_data = [conv_feature_1, conv_feature_2, ..., conv_feature_n, dense_feature_1, ..., dense_feature_m]\n",
    "# where the features of the layers would look like this:\n",
    "# conv_feature_x ----> length = num_filters\n",
    "#                ----> array_entry = 1-D array of images\n",
    "#                ----> each filter will have the same label \n",
    "# dense_feature_x ---> length = num_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c29627a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data length: 28709\n",
      "training data length: 3589\n",
      "testing data length: 3589\n"
     ]
    }
   ],
   "source": [
    "print(f'validation data length: {len(X_train)}')\n",
    "print(f'training data length: {int(0.5*len(X_test))}')\n",
    "print(f'testing data length: {int(0.5*len(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11c53d91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training set...\n",
      "0% | 0/28709\n",
      "1% | 288/28709\n",
      "2% | 575/28709\n",
      "3% | 862/28709\n",
      "4% | 1149/28709\n",
      "5% | 1436/28709\n",
      "6% | 1723/28709\n",
      "7% | 2010/28709\n",
      "8% | 2297/28709\n",
      "9% | 2584/28709\n",
      "10% | 2871/28709\n",
      "11% | 3158/28709\n",
      "12% | 3446/28709\n",
      "13% | 3733/28709\n",
      "14% | 4020/28709\n",
      "15% | 4307/28709\n",
      "16% | 4594/28709\n",
      "17% | 4881/28709\n",
      "18% | 5168/28709\n",
      "19% | 5455/28709\n",
      "20% | 5742/28709\n",
      "21% | 6029/28709\n",
      "22% | 6316/28709\n",
      "23% | 6604/28709\n",
      "24% | 6891/28709\n",
      "25% | 7178/28709\n",
      "26% | 7465/28709\n",
      "27% | 7752/28709\n",
      "28% | 8039/28709\n",
      "29% | 8326/28709\n",
      "30% | 8613/28709\n",
      "31% | 8900/28709\n",
      "32% | 9187/28709\n",
      "33% | 9474/28709\n",
      "34% | 9762/28709\n",
      "35% | 10049/28709\n",
      "36% | 10336/28709\n",
      "37% | 10623/28709\n",
      "38% | 10910/28709\n",
      "39% | 11197/28709\n",
      "40% | 11484/28709\n",
      "41% | 11771/28709\n",
      "42% | 12058/28709\n",
      "43% | 12345/28709\n",
      "44% | 12632/28709\n",
      "45% | 12920/28709\n",
      "46% | 13207/28709\n",
      "47% | 13494/28709\n",
      "48% | 13781/28709\n",
      "49% | 14068/28709\n",
      "50% | 14355/28709\n",
      "51% | 14642/28709\n",
      "52% | 14929/28709\n",
      "53% | 15216/28709\n",
      "54% | 15503/28709\n",
      "55% | 15790/28709\n",
      "56% | 16078/28709\n",
      "57% | 16365/28709\n",
      "58% | 16652/28709\n",
      "59% | 16939/28709\n",
      "60% | 17226/28709\n",
      "61% | 17513/28709\n",
      "62% | 17800/28709\n",
      "63% | 18087/28709\n",
      "64% | 18374/28709\n",
      "65% | 18661/28709\n",
      "66% | 18948/28709\n",
      "67% | 19236/28709\n",
      "68% | 19523/28709\n",
      "69% | 19810/28709\n",
      "70% | 20097/28709\n",
      "71% | 20384/28709\n",
      "72% | 20671/28709\n",
      "73% | 20958/28709\n",
      "74% | 21245/28709\n",
      "75% | 21532/28709\n",
      "76% | 21819/28709\n",
      "77% | 22106/28709\n",
      "78% | 22394/28709\n",
      "79% | 22681/28709\n",
      "80% | 22968/28709\n",
      "81% | 23255/28709\n",
      "82% | 23542/28709\n",
      "83% | 23829/28709\n",
      "84% | 24116/28709\n",
      "85% | 24403/28709\n",
      "86% | 24690/28709\n",
      "87% | 24977/28709\n",
      "88% | 25264/28709\n",
      "89% | 25552/28709\n",
      "90% | 25839/28709\n",
      "91% | 26126/28709\n",
      "92% | 26413/28709\n",
      "93% | 26700/28709\n",
      "94% | 26987/28709\n",
      "95% | 27274/28709\n",
      "96% | 27561/28709\n",
      "97% | 27848/28709\n",
      "98% | 28135/28709\n",
      "99% | 28422/28709\n",
      "Finished generating training set!\n"
     ]
    }
   ],
   "source": [
    "# creating training set\n",
    "train_data = []\n",
    "train_percent = -1\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "print('Generating training set...')\n",
    "\n",
    "for j in range(train_size):\n",
    "    #Calculate new progress percentage\n",
    "    newPercent = int((j/train_size) * 100)\n",
    "    if newPercent != train_percent:\n",
    "        train_percent = newPercent\n",
    "        print(f'{train_percent}% | {j}/{train_size}')\n",
    "\n",
    "    # adding an additional dimension to image (1,dim_1,dim_2)\n",
    "    img = X_train[j].reshape(1, X_train[j].shape[0], X_train[j].shape[1])\n",
    "    # extracting features from the images in training set\n",
    "    features_train = feature_map_model.predict(img,verbose=None)\n",
    "    for k in range(len(features_train)):\n",
    "        if len(features_train[k].shape) == 4:       # extract features from conv2d layers\n",
    "            conv_features = []\n",
    "            for l in range(features_train[k].shape[3]):  # append all conv2d filters (32 or 64 filters) to our conv_features array\n",
    "                conv_features.append( features_train[k][0,:,:,l].ravel() )\n",
    "            # flattening filter vectors\n",
    "            conv_features = np.array(conv_features).ravel()\n",
    "            # storing our feature vectors\n",
    "            train_data.append( (conv_features, y_train[j], X_train[j]) )\n",
    "\n",
    "        elif len(features_train[k].shape) == 2:     # extract features from dense layers\n",
    "            dense_features = features_train[k][0]\n",
    "            train_data.append( (dense_features, y_train[j], X_train[j]) )\n",
    "\n",
    "print('Finished generating training set!')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9184db84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test set...\n",
      "0% | 0/7178\n",
      "1% | 72/7178\n",
      "2% | 144/7178\n",
      "3% | 216/7178\n",
      "4% | 288/7178\n",
      "5% | 359/7178\n",
      "6% | 431/7178\n",
      "7% | 503/7178\n",
      "8% | 575/7178\n",
      "9% | 647/7178\n",
      "10% | 718/7178\n",
      "11% | 790/7178\n",
      "12% | 862/7178\n",
      "13% | 934/7178\n",
      "14% | 1005/7178\n",
      "15% | 1077/7178\n",
      "16% | 1149/7178\n",
      "17% | 1221/7178\n",
      "18% | 1293/7178\n",
      "19% | 1364/7178\n",
      "20% | 1436/7178\n",
      "21% | 1508/7178\n",
      "22% | 1580/7178\n",
      "23% | 1651/7178\n",
      "24% | 1723/7178\n",
      "25% | 1795/7178\n",
      "26% | 1867/7178\n",
      "27% | 1939/7178\n",
      "28% | 2010/7178\n",
      "29% | 2082/7178\n",
      "30% | 2154/7178\n",
      "31% | 2226/7178\n",
      "32% | 2297/7178\n",
      "33% | 2369/7178\n",
      "34% | 2441/7178\n",
      "35% | 2513/7178\n",
      "36% | 2585/7178\n",
      "37% | 2656/7178\n",
      "38% | 2728/7178\n",
      "39% | 2800/7178\n",
      "40% | 2872/7178\n",
      "41% | 2943/7178\n",
      "42% | 3015/7178\n",
      "43% | 3087/7178\n",
      "44% | 3159/7178\n",
      "45% | 3231/7178\n",
      "46% | 3302/7178\n",
      "47% | 3374/7178\n",
      "48% | 3446/7178\n",
      "49% | 3518/7178\n",
      "50% | 3589/7178\n",
      "51% | 3661/7178\n",
      "52% | 3733/7178\n",
      "53% | 3805/7178\n",
      "54% | 3877/7178\n",
      "55% | 3948/7178\n",
      "56% | 4020/7178\n",
      "57% | 4092/7178\n",
      "58% | 4164/7178\n",
      "59% | 4236/7178\n",
      "60% | 4307/7178\n",
      "61% | 4379/7178\n",
      "62% | 4451/7178\n",
      "63% | 4523/7178\n",
      "64% | 4594/7178\n",
      "65% | 4666/7178\n",
      "66% | 4738/7178\n",
      "67% | 4810/7178\n",
      "68% | 4882/7178\n",
      "69% | 4953/7178\n",
      "70% | 5025/7178\n",
      "71% | 5097/7178\n",
      "72% | 5169/7178\n",
      "73% | 5240/7178\n",
      "74% | 5312/7178\n",
      "75% | 5384/7178\n",
      "76% | 5456/7178\n",
      "77% | 5528/7178\n",
      "78% | 5599/7178\n",
      "79% | 5671/7178\n",
      "80% | 5743/7178\n",
      "81% | 5815/7178\n",
      "82% | 5886/7178\n",
      "83% | 5958/7178\n",
      "84% | 6030/7178\n",
      "85% | 6102/7178\n",
      "86% | 6174/7178\n",
      "87% | 6245/7178\n",
      "88% | 6317/7178\n",
      "89% | 6389/7178\n",
      "90% | 6461/7178\n",
      "91% | 6532/7178\n",
      "92% | 6604/7178\n",
      "93% | 6676/7178\n",
      "94% | 6748/7178\n",
      "95% | 6820/7178\n",
      "96% | 6891/7178\n",
      "97% | 6963/7178\n",
      "98% | 7035/7178\n",
      "99% | 7107/7178\n",
      "Finished generating test sets!\n"
     ]
    }
   ],
   "source": [
    "# creating the testing and validation dataset (50/50 split)\n",
    "testing_data = []\n",
    "val_data = []\n",
    "testing_percent = -1\n",
    "testing_size = X_test.shape[0]\n",
    "data_split = int(0.5*len(X_test))\n",
    "\n",
    "print('Generating test set...')\n",
    "\n",
    "for j in range(testing_size):\n",
    "    #Calculate new progress percentage\n",
    "    newPercent = int((j/testing_size) * 100)\n",
    "    if newPercent != testing_percent:\n",
    "        testing_percent = newPercent\n",
    "        print(f'{testing_percent}% | {j}/{testing_size}')\n",
    "        \n",
    "    # adding an additional dimension to image (1,dim_1,dim_2)\n",
    "    img = X_test[j].reshape(1, X_test[j].shape[0], X_test[j].shape[1])\n",
    "    # extracting features from the images in training set\n",
    "    features_test = feature_map_model.predict(img,verbose=None)\n",
    "    for k in range(len(features_test)):\n",
    "        if len(features_test[k].shape) == 4:       # extract features from conv2d layers\n",
    "            conv_features = []\n",
    "            for l in range(features_test[k].shape[3]):  # append all conv2d filters (32 or 64 filters) to our conv_features array\n",
    "                conv_features.append( features_test[k][0,:,:,l].ravel() )\n",
    "            # flattening filter vectors\n",
    "            conv_features = np.array(conv_features).ravel()\n",
    "            # storing our feature vectors\n",
    "            if j < data_split:    #fill the test set\n",
    "                testing_data.append( (conv_features, y_test[j], X_test[j]) )\n",
    "            else:    # fill the validation set\n",
    "                val_data.append( (conv_features, y_test[j], X_test[j]) )\n",
    "\n",
    "        elif len(features_test[k].shape) == 2:     # extract features from dense layers\n",
    "            dense_features = features_test[k][0]\n",
    "            if j < data_split:    #fill the test set\n",
    "                testing_data.append( (dense_features, y_test[j], X_test[j]) )\n",
    "            else:    # fill the validation set\n",
    "                val_data.append( (dense_features, y_test[j], X_test[j]) )\n",
    "            \n",
    "print('Finished generating test sets!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be9d97c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03288623, -0.01935673, -0.01263739, ...,  0.00850673,\n",
       "         0.00559484,  0.00593166], dtype=float32),\n",
       " 5,\n",
       " array([[0.70196078, 0.85490196, 0.92941176, ..., 0.65882353, 0.64705882,\n",
       "         0.62745098],\n",
       "        [0.69411765, 0.85882353, 0.94117647, ..., 0.66666667, 0.63921569,\n",
       "         0.63137255],\n",
       "        [0.69019608, 0.85098039, 0.9372549 , ..., 0.68235294, 0.64313725,\n",
       "         0.63529412],\n",
       "        ...,\n",
       "        [0.56470588, 0.65490196, 0.69803922, ..., 0.12941176, 0.08627451,\n",
       "         0.03921569],\n",
       "        [0.56078431, 0.65098039, 0.69411765, ..., 0.10980392, 0.06666667,\n",
       "         0.03921569],\n",
       "        [0.56078431, 0.64705882, 0.69411765, ..., 0.07843137, 0.04705882,\n",
       "         0.04705882]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "929e2f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0,1000,27):\n",
    "#     print(f'{i}.) train_data_shape = {train_data[i][0].shape}, label = {train_data[i][1]}', end='\\n\\n')\n",
    "#     print(train_data[i][1],end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ac6e805",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4BzyhcpMJJ4",
    "outputId": "41a18cab-7a26-4a16-fd06-3d619108a946",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WARNING: DO NOT run this cell unless you want to kill your kernel!!! Data is too big (7GB) :)\n",
    "# np_train_data = np.array(train_data)\n",
    "# np.save('GBD_train_data',np_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4edb6044",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_datasets(data, j, layer_names, dtype):\n",
    "    # next create a function that separates the data into datasets per layer\n",
    "    # input arguments are:\n",
    "        # --> data = an array of tuples arranged as (data, label)\n",
    "        # --> j = index to keep track of the layer name\n",
    "        # --> layer_names = array with the names of layers of our CNNs\n",
    "        # --> dtype = string with type of data we want to create (train, test, validation)\n",
    "    # output will be the data saved as .npy files in their respective folders\n",
    "    \n",
    "    # only save layers\n",
    "    if j < 20:\n",
    "        return\n",
    "    \n",
    "    # i will keep track of the indices of train_data and be updated by +27\n",
    "    i = j\n",
    "    output_data = []\n",
    "    output_labels = []\n",
    "    output_imgs = []\n",
    "    \n",
    "    while i < len(data):\n",
    "        # will try to save dataset with layer_name files\n",
    "        output_data.append( data[i][0] )\n",
    "        output_labels.append( data[i][1] )\n",
    "        output_imgs.append( data[i][2] )\n",
    "        i+=27\n",
    "    file_data = f'./GBT_data/{dtype}/data/{layer_names[j]}_layer_{j}_data'\n",
    "    file_labels = f'./GBT_data/{dtype}/labels/{layer_names[j]}_layer_{j}_labels'\n",
    "    np.save(file_data, output_data)\n",
    "    np.save(file_labels, output_labels)\n",
    "    \n",
    "    if(j == 26):\n",
    "        file_imgs = f'./GBT_data/{dtype}/labels/img'\n",
    "        np.save(file_imgs, output_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "580254e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(layer_names)):\n",
    "    create_datasets(train_data, j, layer_names, dtype='train')\n",
    "    create_datasets(val_data, j, layer_names, dtype='validation')\n",
    "    create_datasets(testing_data, j, layer_names, dtype='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29148fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20096.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bab01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
